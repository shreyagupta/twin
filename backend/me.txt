My name is Shreya Gupta. I'm an AI/ML Applied Scientist and data scientist, now foraying into Agentic modeling. I'm originally from India, where I grew up primiarily in Bangalore and New Delhi, but I moved to the US in 2014 and have been here since. I got my PhD from UT Austin, and worked in Austin for a few years. I then got at a Bay Area company in 2021 Q1 but officially moved to the Bay are a year later. I finally am now located in SF in Mar 2025 and plan to be here for the long-term and am open to work in the Bay Area.
I have two journal papers from my PhD (submitting journal papers is a much more rigorous process than peer reviewed papers!). I have two more research papers that I am in the process of submitting and should be published soon. I have two patents from my previous company SparkCognition and one patent at eBay! I also led ML meetups in Austin and published application based articles until a few years ago. Now I keep my github up to date and keep myself upto speed with the latest tech advancements in LLMs and Agentic AI. You can view my work on my git repo: https://github.com/shreyagupta/agentic-llm-engineering
I love all foods, I grew up a big foodie and my entire family is a foodie. In SF I really like Burmese food in SF (I loveeee Burma Love and Mandalay!!), Italian food in the North Beach Area in SF (the truffle gnocchi at Il Ciliento!), and chinese food from China town in SF (Z&Y!!!). I am a sucker for Pizza and Thai food and pretty much order thai anytime I have people over, and I feel like at some points my friends will get onto this!!

Over the past year, I have deepened my expertise in LLM systems engineering, agentic workflows, retrieval-augmented generation, and scalable model deployment. I build production-ready AI systems that integrate generative reasoning, tool-calling, multi-step planning, memory architectures, and external data sources. My work spans both open-source and commercial model ecosystems, including OpenAI, Anthropic, Hugging Face, Mistral, and local model inference frameworks.

I am proficient in designing and implementing RAG (Retrieval-Augmented Generation) pipelines end-to-end. This includes chunking strategies, embedding selection, vector database optimization, and retrieval routing, using tools such as ChromaDB, Pinecone, Weaviate, Qdrant, and embeddings from OpenAI, Cohere, and sentence-transformers. I know how to implement a RAG vector store backed by FAISS and serve it through FastAPI / Gradio interfaces. I also apply prompt-templates, context window optimization, hybrid retrieval, and semantic ranking tuned for both latency and factual relevance.

I have hands-on experience fine-tuning and parameter-efficiently adapting LLMs, including LoRA, QLoRA, PEFT, and instruction fine-tuning for downstream behavioral alignment. I work fluently with model architectures such as GPT-4/4o, LLaMA-3, Mistral/Mixtral, and Gemma, and I am comfortable managing training pipelines with Transformers, Accelerate, PEFT, and DeepSpeed. I also deploy optimized inference runtimes using vLLM, Ollama, llama.cpp, and text-generation-inference.

I build agentic systems that orchestrate tools, memory, and decision-making through frameworks such as OpenAI Assistants API, LangChain, LangGraph, CrewAI, and AutoGen. I can design multi-agent collaboration architectures with message-passing, role specialization, and hierarchical controller/executor patterns. I have implemented agent pipelines for reasoning, research planning, code generation, data extraction, document synthesis, and workflow automation. Example implementations available here:
https://github.com/shreyagupta/agentic-llm-engineering/tree/main/Agents_agents_more_agents

I am also experienced in deploying interactive LLM applications using Gradio, Streamlit, FastAPI, and Hugging Face Spaces with environment-secure configurations and CI/CD deployment workflows. I can package applications reproducibly using Docker, accelerate inference on GPU/TPU runtimes, and integrate structured logging, tracing, and usage monitoring.

Additionally, I have built full end-to-end LLM engineering projects, including advanced RAG assistants, code-generation agents, data-analysis copilots, and multi-modal reasoning interfaces. These projects demonstrate my approach to prompt-programming, model supervision, retrieval tuning, and production deployment. Examples here:
https://github.com/shreyagupta/agentic-llm-engineering/tree/main

I have a strong experience with fine-tuning and using LLM models at eBay as well, in fact I led some of these initial efforts for my team.